{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/tareen/Desktop/Research_Projects/2020_mavenn_github/mavenn_local')\n",
    "\n",
    "import mavenn\n",
    "import logomaker\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test N: 10,269\n",
      "training + validation N: 40,249\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set</th>\n",
       "      <th>ct_0</th>\n",
       "      <th>ct_1</th>\n",
       "      <th>ct_2</th>\n",
       "      <th>ct_3</th>\n",
       "      <th>ct_4</th>\n",
       "      <th>ct_5</th>\n",
       "      <th>ct_6</th>\n",
       "      <th>ct_7</th>\n",
       "      <th>ct_8</th>\n",
       "      <th>ct_9</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>validation</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AATTGATGTCCGGTAGCTCACTCATTAGGCAGCCAAGGTTTTAGAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AATTCATGTGAGTTATGTCTCTCATTAGGCACCCCAGGCTTGAGAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AATTAATGTGAGTTAGCTCACTCATTGGACACCCCAGGCTTTACAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>training</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ACTTAATAAAAGTCAGCTCACTCATTAATCACCCCACGCTCTACAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>training</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AATTAATGTGAGTTATCTAACTCATTAGGCACCCCAGGCTTTACAC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          set  ct_0  ct_1  ct_2  ct_3  ct_4  ct_5  ct_6  ct_7  ct_8  ct_9  \\\n",
       "0  validation     2     0     0     0     0     0     0     0     0     0   \n",
       "1    training     0     0     0     1     0     0     0     0     0     0   \n",
       "2    training     1     0     0     0     0     0     0     0     0     0   \n",
       "3    training     0     1     0     0     0     0     0     0     0     0   \n",
       "4    training     0     0     0     0     0     3     0     0     0     0   \n",
       "\n",
       "                                                   x  \n",
       "0  AATTGATGTCCGGTAGCTCACTCATTAGGCAGCCAAGGTTTTAGAC...  \n",
       "1  AATTCATGTGAGTTATGTCTCTCATTAGGCACCCCAGGCTTGAGAT...  \n",
       "2  AATTAATGTGAGTTAGCTCACTCATTGGACACCCCAGGCTTTACAC...  \n",
       "3  ACTTAATAAAAGTCAGCTCACTCATTAATCACCCCACGCTCTACAT...  \n",
       "4  AATTAATGTGAGTTATCTAACTCATTAGGCACCCCAGGCTTTACAC...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load example data\n",
    "data_df = mavenn.load_example_dataset('sortseq_full-wt')\n",
    "\n",
    "# Separate test from data_df\n",
    "ix_test = data_df['set']=='test'\n",
    "test_df = data_df[ix_test].reset_index(drop=True)\n",
    "print(f'test N: {len(test_df):,}')\n",
    "\n",
    "# Remove test data from data_df\n",
    "data_df = data_df[~ix_test].reset_index(drop=True)\n",
    "print(f'training + validation N: {len(data_df):,}')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=75, Y=10\n"
     ]
    }
   ],
   "source": [
    "# Comptue sequence length and number of bins\n",
    "L = len(data_df['x'][0])\n",
    "y_cols = data_df.columns[1:-1]\n",
    "Y = len(y_cols)\n",
    "print(f'L={L}, Y={Y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mavenn.src.layers.gpmap import CustomGPMapLayer\n",
    "\n",
    "\n",
    "# Tensorflow imports\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Layer, Dense\n",
    "\n",
    "class ThermodynamicLayer(CustomGPMapLayer):\n",
    "    \"\"\"Represents an thermodynamic model of transcription\n",
    "       regulation in E. Coli at the lac promoter, which \n",
    "       contains binding sites for RNAP and CRP.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 L_crp,\n",
    "                 L_rnap,\n",
    "                 C,\n",
    "                 regularizer, \n",
    "                 *args, **kwargs):\n",
    "        \"\"\"Construct layer instance.\"\"\"\n",
    "        self.L_crp=L_crp\n",
    "        self.L_rnap=L_rnap\n",
    "        self.C=C\n",
    "        self.regularizer = tf.keras.regularizers.L2(regularizer)\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Build layer.\"\"\"\n",
    "        \n",
    "        # define bias/chemical potential weight for crp\n",
    "        self.mu_crp = self.add_weight(name='mu_crp',\n",
    "                                       shape=(1,),\n",
    "                                       initializer=Constant(0.),\n",
    "                                       trainable=True,\n",
    "                                       regularizer=self.regularizer)\n",
    "\n",
    "        # define bias/chemical potential weight for rnap\n",
    "        self.mu_rnap = self.add_weight(name='mu_rnap',\n",
    "                                       shape=(1,),\n",
    "                                       initializer=Constant(0.),\n",
    "                                       trainable=True,\n",
    "                                       regularizer=self.regularizer)\n",
    "\n",
    "\n",
    "        # Define theta_crp_lc parameters\n",
    "        theta_crp_lc_shape = (1, self.L_crp, self.C)\n",
    "\n",
    "        theta_crp_lc_init = np.random.randn(*theta_crp_lc_shape)/np.sqrt(self.L_crp)\n",
    "        self.theta_crp_lc = self.add_weight(name='theta_crp_lc',\n",
    "                                        shape=theta_crp_lc_shape,\n",
    "                                        initializer=Constant(theta_crp_lc_init),\n",
    "                                        trainable=True,\n",
    "                                        regularizer=self.regularizer)\n",
    "        \n",
    "        # Define theta_rnap_lc parameters\n",
    "        theta_rnap_lc_shape = (1, self.L_rnap, self.C)\n",
    "\n",
    "        theta_rnap_lc_init = np.random.randn(*theta_rnap_lc_shape)/np.sqrt(self.L_rnap)\n",
    "        self.theta_rnap_lc = self.add_weight(name='theta_rnap_lc',\n",
    "                                        shape=theta_rnap_lc_shape,\n",
    "                                        initializer=Constant(theta_rnap_lc_init),\n",
    "                                        trainable=True,\n",
    "                                        regularizer=self.regularizer)\n",
    "        \n",
    "        # define interaction term. Not sure if this needs regularization\n",
    "        self.interaction = self.add_weight(name='interaction',\n",
    "                               shape=(1,),\n",
    "                               initializer=Constant(0.),\n",
    "                               trainable=True,\n",
    "                               regularizer=self.regularizer)\n",
    "        \n",
    "        # define tsat term. Not sure if this needs regularization\n",
    "        self.tsat = self.add_weight(name='tsat',\n",
    "                               shape=(1,),\n",
    "                               initializer=Constant(0.),\n",
    "                               trainable=True,\n",
    "                               regularizer=self.regularizer)        \n",
    "        \n",
    "        # Call superclass build\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x_lc):\n",
    "        \"\"\"Process layer input and return output.\n",
    "        \n",
    "        x_lc: (tensor)\n",
    "            Input tensor that represents one-hot encoded \n",
    "            sequence values. \n",
    "        \"\"\"\n",
    "        \n",
    "        # extract locations of binding sites from entire lac-promoter sequence.\n",
    "        x_crp_lc = x_lc[:,4:108]\n",
    "        x_rnap_lc = x_lc[:,136:300]\n",
    "        \n",
    "        # reshape according to crp and rnap lengths. \n",
    "        x_crp_lc = tf.reshape(x_crp_lc, [-1, self.L_crp, self.C])\n",
    "        x_rnap_lc = tf.reshape(x_rnap_lc, [-1, self.L_rnap, self.C])\n",
    "                \n",
    "        # compute delta G for crp    \n",
    "        phi_crp = self.mu_crp + \\\n",
    "              tf.reshape(K.sum(self.theta_crp_lc * x_crp_lc, axis=[1, 2]),\n",
    "                         shape=[-1, 1])\n",
    "            \n",
    "        # compute delta G for rnap\n",
    "        phi_rnap = self.mu_rnap + \\\n",
    "              tf.reshape(K.sum(self.theta_rnap_lc * x_rnap_lc, axis=[1, 2]),\n",
    "                         shape=[-1, 1])            \n",
    "        \n",
    "        # compute rate of transcription\n",
    "        t = (self.tsat)*(K.exp(-phi_crp)+K.exp(-phi_rnap)+K.exp(-phi_crp-phi_rnap-self.interaction))/(1+K.exp(-phi_crp)+K.exp(-phi_rnap)+K.exp(-phi_crp-phi_rnap-self.interaction))\n",
    "        \n",
    "        # return rate of transcription\n",
    "        return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define custom gp_map parameters dictionary\n",
    "gpmap_kwargs = {'L_crp':26,\n",
    "                'L_rnap':41,\n",
    "                'C':4,\n",
    "                'regularizer':0.005}\n",
    "\n",
    "# Create model\n",
    "model = mavenn.Model(L=L, \n",
    "                     Y=Y,\n",
    "                     alphabet='dna', \n",
    "                     regression_type='MPA', \n",
    "                     gpmap_type='custom',\n",
    "                     gpmap_kwargs=gpmap_kwargs,\n",
    "                     custom_gpmap=ThermodynamicLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 40,249 observations set as training data.\n",
      "Using 24.7% for validation.\n",
      "Data shuffled.\n",
      "Time to set data: 0.608 sec.\n",
      "Warning: linear initialization has no effect when gpmap_type=custom.\n",
      "Epoch 1/2000\n",
      "152/152 [==============================] - 2s 6ms/step - loss: 663.7367 - I_var: -0.0015 - val_loss: 665.3579 - val_I_var: -6.4517e-04\n",
      "Epoch 2/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 660.0104 - I_var: -3.8917e-04 - val_loss: 658.8937 - val_I_var: -3.0497e-04\n",
      "Epoch 3/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 651.1711 - I_var: -1.6168e-04 - val_loss: 653.1945 - val_I_var: -1.3896e-04\n",
      "Epoch 4/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 646.2539 - I_var: 8.6601e-05 - val_loss: 648.0873 - val_I_var: 6.2466e-04\n",
      "Epoch 5/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 641.2505 - I_var: 0.0022 - val_loss: 642.5858 - val_I_var: 0.0073\n",
      "Epoch 6/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 633.8534 - I_var: 0.0335 - val_loss: 625.8810 - val_I_var: 0.0823\n",
      "Epoch 7/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 611.0764 - I_var: 0.1579 - val_loss: 597.7230 - val_I_var: 0.2310\n",
      "Epoch 8/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 586.2865 - I_var: 0.2898 - val_loss: 579.7689 - val_I_var: 0.3277\n",
      "Epoch 9/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 569.7258 - I_var: 0.3608 - val_loss: 570.7659 - val_I_var: 0.3750\n",
      "Epoch 10/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 557.1015 - I_var: 0.3983 - val_loss: 565.0934 - val_I_var: 0.4040\n",
      "Epoch 11/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 563.2359 - I_var: 0.4217 - val_loss: 561.4349 - val_I_var: 0.4222\n",
      "Epoch 12/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 558.2197 - I_var: 0.4361 - val_loss: 558.6641 - val_I_var: 0.4357\n",
      "Epoch 13/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 553.3863 - I_var: 0.4481 - val_loss: 556.4202 - val_I_var: 0.4465\n",
      "Epoch 14/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 553.5154 - I_var: 0.4565 - val_loss: 555.5917 - val_I_var: 0.4501\n",
      "Epoch 15/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 548.4507 - I_var: 0.4626 - val_loss: 553.5322 - val_I_var: 0.4603\n",
      "Epoch 16/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 547.5983 - I_var: 0.4686 - val_loss: 552.6196 - val_I_var: 0.4643\n",
      "Epoch 17/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 552.0466 - I_var: 0.4733 - val_loss: 552.8029 - val_I_var: 0.4626\n",
      "Epoch 18/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 547.3244 - I_var: 0.4774 - val_loss: 550.5975 - val_I_var: 0.4736\n",
      "Epoch 19/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 545.4295 - I_var: 0.4812 - val_loss: 550.1738 - val_I_var: 0.4751\n",
      "Epoch 20/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 543.7261 - I_var: 0.4835 - val_loss: 549.4746 - val_I_var: 0.4784\n",
      "Epoch 21/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 542.0472 - I_var: 0.4864 - val_loss: 548.5128 - val_I_var: 0.4827\n",
      "Epoch 22/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 545.9265 - I_var: 0.4885 - val_loss: 547.9742 - val_I_var: 0.4850\n",
      "Epoch 23/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 545.0135 - I_var: 0.4920 - val_loss: 548.3331 - val_I_var: 0.4824\n",
      "Epoch 24/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 546.5700 - I_var: 0.4938 - val_loss: 547.4156 - val_I_var: 0.4868\n",
      "Epoch 25/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 543.1351 - I_var: 0.4951 - val_loss: 546.6752 - val_I_var: 0.4900\n",
      "Epoch 26/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 540.1230 - I_var: 0.4970 - val_loss: 546.1649 - val_I_var: 0.4921\n",
      "Epoch 27/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 541.3641 - I_var: 0.4986 - val_loss: 545.7482 - val_I_var: 0.4937\n",
      "Epoch 28/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 541.1238 - I_var: 0.4996 - val_loss: 545.4034 - val_I_var: 0.4951\n",
      "Epoch 29/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 539.7293 - I_var: 0.5020 - val_loss: 544.9760 - val_I_var: 0.4966\n",
      "Epoch 30/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 541.5740 - I_var: 0.5027 - val_loss: 544.6477 - val_I_var: 0.4980\n",
      "Epoch 31/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 540.5726 - I_var: 0.5039 - val_loss: 544.2332 - val_I_var: 0.4996\n",
      "Epoch 32/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 539.8222 - I_var: 0.5053 - val_loss: 544.0715 - val_I_var: 0.4999\n",
      "Epoch 33/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 537.8252 - I_var: 0.5059 - val_loss: 543.7961 - val_I_var: 0.5008\n",
      "Epoch 34/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 536.2919 - I_var: 0.5077 - val_loss: 543.2725 - val_I_var: 0.5031\n",
      "Epoch 35/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 534.0860 - I_var: 0.5085 - val_loss: 543.0248 - val_I_var: 0.5038\n",
      "Epoch 36/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 540.0433 - I_var: 0.5086 - val_loss: 543.3347 - val_I_var: 0.5016\n",
      "Epoch 37/2000\n",
      "152/152 [==============================] - 1s 4ms/step - loss: 539.6878 - I_var: 0.5107 - val_loss: 542.4401 - val_I_var: 0.5058\n",
      "Epoch 38/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 537.4544 - I_var: 0.5107 - val_loss: 542.1479 - val_I_var: 0.5069\n",
      "Epoch 39/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 537.9335 - I_var: 0.5123 - val_loss: 542.0443 - val_I_var: 0.5071\n",
      "Epoch 40/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 537.2568 - I_var: 0.5128 - val_loss: 541.6028 - val_I_var: 0.5089\n",
      "Epoch 41/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 534.6043 - I_var: 0.5141 - val_loss: 541.3960 - val_I_var: 0.5096\n",
      "Epoch 42/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 535.4463 - I_var: 0.5140 - val_loss: 541.1525 - val_I_var: 0.5104\n",
      "Epoch 43/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 538.2357 - I_var: 0.5155 - val_loss: 540.9454 - val_I_var: 0.5111\n",
      "Epoch 44/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 535.9329 - I_var: 0.5164 - val_loss: 540.9072 - val_I_var: 0.5108\n",
      "Epoch 45/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 537.7077 - I_var: 0.5157 - val_loss: 540.4080 - val_I_var: 0.5130\n",
      "Epoch 46/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 535.7709 - I_var: 0.5180 - val_loss: 540.5449 - val_I_var: 0.5119\n",
      "Epoch 47/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 536.2611 - I_var: 0.5187 - val_loss: 540.2071 - val_I_var: 0.5133\n",
      "Epoch 48/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 535.6259 - I_var: 0.5200 - val_loss: 540.0336 - val_I_var: 0.5136\n",
      "Epoch 49/2000\n",
      "152/152 [==============================] - 1s 3ms/step - loss: 535.6329 - I_var: 0.5187 - val_loss: 540.5878 - val_I_var: 0.5103\n",
      "Epoch 50/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 535.3705 - I_var: 0.5198 - val_loss: 539.4774 - val_I_var: 0.5159\n",
      "Epoch 51/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 534.1896 - I_var: 0.5205 - val_loss: 539.2363 - val_I_var: 0.5168\n",
      "Epoch 52/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 532.9009 - I_var: 0.5208 - val_loss: 539.2286 - val_I_var: 0.5162\n",
      "Epoch 53/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 534.0176 - I_var: 0.5212 - val_loss: 538.9793 - val_I_var: 0.5174\n",
      "Epoch 54/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 532.5068 - I_var: 0.5219 - val_loss: 538.8596 - val_I_var: 0.5176\n",
      "Epoch 55/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 532.2952 - I_var: 0.5229 - val_loss: 538.6665 - val_I_var: 0.5183\n",
      "Epoch 56/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 534.3956 - I_var: 0.5230 - val_loss: 538.6205 - val_I_var: 0.5182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 528.7666 - I_var: 0.5248 - val_loss: 538.3342 - val_I_var: 0.5193\n",
      "Epoch 58/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 535.7103 - I_var: 0.5246 - val_loss: 538.3080 - val_I_var: 0.5192\n",
      "Epoch 59/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 533.5565 - I_var: 0.5248 - val_loss: 537.9628 - val_I_var: 0.5207\n",
      "Epoch 60/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 531.0642 - I_var: 0.5246 - val_loss: 537.8702 - val_I_var: 0.5207\n",
      "Epoch 61/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 531.3769 - I_var: 0.5254 - val_loss: 537.6061 - val_I_var: 0.5218\n",
      "Epoch 62/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 534.9054 - I_var: 0.5262 - val_loss: 537.6574 - val_I_var: 0.5212\n",
      "Epoch 63/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 530.5363 - I_var: 0.5267 - val_loss: 537.3468 - val_I_var: 0.5226\n",
      "Epoch 64/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 531.5805 - I_var: 0.5259 - val_loss: 537.1713 - val_I_var: 0.5232\n",
      "Epoch 65/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 532.5896 - I_var: 0.5277 - val_loss: 537.1947 - val_I_var: 0.5229\n",
      "Epoch 66/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 531.9162 - I_var: 0.5285 - val_loss: 537.1535 - val_I_var: 0.5227\n",
      "Epoch 67/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 533.0032 - I_var: 0.5285 - val_loss: 537.2952 - val_I_var: 0.5217\n",
      "Epoch 68/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 532.1772 - I_var: 0.5295 - val_loss: 537.3779 - val_I_var: 0.5210\n",
      "Epoch 69/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 535.4316 - I_var: 0.5301 - val_loss: 537.1544 - val_I_var: 0.5219\n",
      "Epoch 70/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 533.4522 - I_var: 0.5299 - val_loss: 536.6213 - val_I_var: 0.5245\n",
      "Epoch 71/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 529.2573 - I_var: 0.5308 - val_loss: 536.4116 - val_I_var: 0.5253\n",
      "Epoch 72/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 532.2454 - I_var: 0.5304 - val_loss: 536.2686 - val_I_var: 0.5258\n",
      "Epoch 73/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 532.4345 - I_var: 0.5316 - val_loss: 536.1822 - val_I_var: 0.5261\n",
      "Epoch 74/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 529.6498 - I_var: 0.5322 - val_loss: 536.2688 - val_I_var: 0.5252\n",
      "Epoch 75/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 529.5454 - I_var: 0.5317 - val_loss: 535.8239 - val_I_var: 0.5272\n",
      "Epoch 76/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 529.3002 - I_var: 0.5323 - val_loss: 535.7663 - val_I_var: 0.5273\n",
      "Epoch 77/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 528.6118 - I_var: 0.5321 - val_loss: 535.6275 - val_I_var: 0.5278\n",
      "Epoch 78/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 529.0794 - I_var: 0.5326 - val_loss: 535.4659 - val_I_var: 0.5285\n",
      "Epoch 79/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 534.1887 - I_var: 0.5332 - val_loss: 535.6218 - val_I_var: 0.5274\n",
      "Epoch 80/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 530.6096 - I_var: 0.5337 - val_loss: 535.1252 - val_I_var: 0.5298\n",
      "Epoch 81/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 531.2444 - I_var: 0.5337 - val_loss: 535.4219 - val_I_var: 0.5281\n",
      "Epoch 82/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 528.6609 - I_var: 0.5346 - val_loss: 535.0079 - val_I_var: 0.5299\n",
      "Epoch 83/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 529.3374 - I_var: 0.5343 - val_loss: 535.4159 - val_I_var: 0.5275\n",
      "Epoch 84/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 534.5154 - I_var: 0.5347 - val_loss: 535.0176 - val_I_var: 0.5296\n",
      "Epoch 85/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 531.1509 - I_var: 0.5352 - val_loss: 534.5977 - val_I_var: 0.5315\n",
      "Epoch 86/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 529.2189 - I_var: 0.5352 - val_loss: 534.5612 - val_I_var: 0.5314\n",
      "Epoch 87/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 529.8351 - I_var: 0.5362 - val_loss: 534.3998 - val_I_var: 0.5322\n",
      "Epoch 88/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 527.2318 - I_var: 0.5365 - val_loss: 534.3773 - val_I_var: 0.5320\n",
      "Epoch 89/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 529.5239 - I_var: 0.5360 - val_loss: 534.5206 - val_I_var: 0.5311\n",
      "Epoch 90/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 527.1069 - I_var: 0.5374 - val_loss: 534.1831 - val_I_var: 0.5325\n",
      "Epoch 91/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 528.2763 - I_var: 0.5366 - val_loss: 534.4595 - val_I_var: 0.5309\n",
      "Epoch 92/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 528.8903 - I_var: 0.5381 - val_loss: 534.6421 - val_I_var: 0.5297\n",
      "Epoch 93/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 530.4453 - I_var: 0.5380 - val_loss: 534.0095 - val_I_var: 0.5329\n",
      "Epoch 94/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 529.9588 - I_var: 0.5378 - val_loss: 534.9261 - val_I_var: 0.5280\n",
      "Epoch 95/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 532.0946 - I_var: 0.5386 - val_loss: 533.8885 - val_I_var: 0.5333\n",
      "Epoch 96/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 527.6527 - I_var: 0.5394 - val_loss: 534.1591 - val_I_var: 0.5315\n",
      "Epoch 97/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 528.8346 - I_var: 0.5387 - val_loss: 533.5728 - val_I_var: 0.5345\n",
      "Epoch 98/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 530.6450 - I_var: 0.5394 - val_loss: 533.4745 - val_I_var: 0.5349\n",
      "Epoch 99/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 527.8985 - I_var: 0.5392 - val_loss: 533.5475 - val_I_var: 0.5341\n",
      "Epoch 100/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 531.8302 - I_var: 0.5394 - val_loss: 533.5958 - val_I_var: 0.5338\n",
      "Epoch 101/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 528.7261 - I_var: 0.5396 - val_loss: 533.2543 - val_I_var: 0.5354\n",
      "Epoch 102/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 525.9579 - I_var: 0.5401 - val_loss: 533.3221 - val_I_var: 0.5350\n",
      "Epoch 103/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 526.7492 - I_var: 0.5394 - val_loss: 533.3221 - val_I_var: 0.5348\n",
      "Epoch 104/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 527.8910 - I_var: 0.5408 - val_loss: 533.1746 - val_I_var: 0.5354\n",
      "Epoch 105/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 527.3391 - I_var: 0.5405 - val_loss: 533.1599 - val_I_var: 0.5352\n",
      "Epoch 106/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 527.0807 - I_var: 0.5422 - val_loss: 532.8641 - val_I_var: 0.5366\n",
      "Epoch 107/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 527.4245 - I_var: 0.5407 - val_loss: 533.2150 - val_I_var: 0.5348\n",
      "Epoch 108/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 524.8087 - I_var: 0.5426 - val_loss: 532.8288 - val_I_var: 0.5366\n",
      "Epoch 109/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 529.3303 - I_var: 0.5427 - val_loss: 532.8156 - val_I_var: 0.5366\n",
      "Epoch 110/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 529.6996 - I_var: 0.5431 - val_loss: 533.0829 - val_I_var: 0.5350\n",
      "Epoch 111/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 523.2445 - I_var: 0.5421 - val_loss: 532.5022 - val_I_var: 0.5378\n",
      "Epoch 112/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 529.1175 - I_var: 0.5422 - val_loss: 532.7734 - val_I_var: 0.5362\n",
      "Epoch 113/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 526.8187 - I_var: 0.5423 - val_loss: 532.3996 - val_I_var: 0.5380\n",
      "Epoch 114/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 0s 2ms/step - loss: 529.2665 - I_var: 0.5433 - val_loss: 532.6874 - val_I_var: 0.5364\n",
      "Epoch 115/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 528.8700 - I_var: 0.5431 - val_loss: 532.2554 - val_I_var: 0.5386\n",
      "Epoch 116/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 527.4465 - I_var: 0.5435 - val_loss: 532.4904 - val_I_var: 0.5372\n",
      "Epoch 117/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 529.0925 - I_var: 0.5423 - val_loss: 532.0795 - val_I_var: 0.5392\n",
      "Epoch 118/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 525.8535 - I_var: 0.5430 - val_loss: 532.2454 - val_I_var: 0.5379\n",
      "Epoch 119/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 530.1305 - I_var: 0.5433 - val_loss: 532.0603 - val_I_var: 0.5389\n",
      "Epoch 120/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 524.9605 - I_var: 0.5442 - val_loss: 532.0541 - val_I_var: 0.5388\n",
      "Epoch 121/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 527.5238 - I_var: 0.5447 - val_loss: 531.9145 - val_I_var: 0.5394\n",
      "Epoch 122/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 529.1112 - I_var: 0.5437 - val_loss: 531.6932 - val_I_var: 0.5406\n",
      "Epoch 123/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 526.6070 - I_var: 0.5440 - val_loss: 532.4362 - val_I_var: 0.5365\n",
      "Epoch 124/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 524.7012 - I_var: 0.5453 - val_loss: 532.5089 - val_I_var: 0.5358\n",
      "Epoch 125/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 526.8555 - I_var: 0.5454 - val_loss: 531.9694 - val_I_var: 0.5386\n",
      "Epoch 126/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 524.8267 - I_var: 0.5456 - val_loss: 533.0739 - val_I_var: 0.5326\n",
      "Epoch 127/2000\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 527.3615 - I_var: 0.5445 - val_loss: 531.5776 - val_I_var: 0.5405\n",
      "Epoch 128/2000\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 526.4147 - I_var: 0.5455 - val_loss: 531.6127 - val_I_var: 0.5402\n",
      "Epoch 129/2000\n",
      " 54/152 [=========>....................] - ETA: 0s - loss: 520.9808 - I_var: 0.5560"
     ]
    }
   ],
   "source": [
    "# Set training data\n",
    "model.set_data(x=data_df['x'],\n",
    "               y=data_df[y_cols],\n",
    "               validation_flags=(data_df['set']=='validation'),\n",
    "               shuffle=True)\n",
    "\n",
    "# Fit model to data\n",
    "model.fit(learning_rate=.0005,\n",
    "          epochs=2000,\n",
    "          batch_size=200,\n",
    "          early_stopping=True,\n",
    "          early_stopping_patience=25,\n",
    "          linear_initialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training history\n",
    "print('On test data:')\n",
    "x_test = test_df['x'].values\n",
    "y_test = test_df[y_cols].values\n",
    "\n",
    "# Compute likelihood information\n",
    "I_var, dI_var =  model.I_variational(x=x_test, y=y_test)\n",
    "print(f'I_var_test: {I_var:.3f} +- {dI_var:.3f} bits') \n",
    "\n",
    "# Compute predictive information\n",
    "I_pred, dI_pred = model.I_predictive(x=x_test, y=y_test)\n",
    "print(f'I_pred_test: {I_pred:.3f} +- {dI_pred:.3f} bits')\n",
    "\n",
    "I_var_hist = model.history['I_var']\n",
    "val_I_var_hist = model.history['val_I_var']\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=[4,4])\n",
    "ax.plot(I_var_hist, label='I_var_train')\n",
    "ax.plot(val_I_var_hist, label='I_var_val')\n",
    "#ax.axhline(I_var, color='C2', linestyle=':', label='I_var_test')\n",
    "#ax.axhline(I_pred, color='C3', linestyle=':', label='I_pred_test')\n",
    "ax.legend()\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('bits')\n",
    "ax.set_title('training hisotry')\n",
    "#ax.set_ylim([0, I_pred*1.2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_term = model.get_nn().layers[2].get_weights()[4]/1.62\n",
    "crp_weights = model.get_nn().layers[2].get_weights()[2][0]\n",
    "rnap_weights = model.get_nn().layers[2].get_weights()[3][0]\n",
    "print(f'interaction term = {interaction_term[0]:.3f} k_cal/mol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crp_df = pd.DataFrame(crp_weights,columns=model.alphabet)\n",
    "rnap_df = pd.DataFrame(rnap_weights,columns=model.alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get G-P map parameters in matrix form\n",
    "# theta = model.get_theta(gauge='uniform')\n",
    "\n",
    "# Create grid in phi space\n",
    "phi_lim = [-4, 4]\n",
    "phi_grid = np.linspace(phi_lim[0], phi_lim[1], 1000)\n",
    "\n",
    "# Create array of allowable y values\n",
    "Y = model.model.Y    # Y = number of bins\n",
    "y_lim = [-.5, Y-.5]\n",
    "y_all = range(Y)\n",
    "\n",
    "# Compute matrix of p(y|phi) values\n",
    "measurement_process = model.p_of_y_given_phi(y_all, phi_grid)\n",
    "\n",
    "# # Create figure with two panels\n",
    "fig, axs = plt.subplots(1,3,figsize=[18,4])\n",
    "\n",
    "logomaker.Logo(-crp_df,ax=axs[0],center_values=True)\n",
    "logomaker.Logo(-rnap_df,ax=axs[1],center_values=True)\n",
    "\n",
    "# # Right panel: draw measurement process as heatmap\n",
    "ax = axs[2]\n",
    "im = ax.imshow(measurement_process,\n",
    "               cmap='Greens',\n",
    "               extent=phi_lim+y_lim,\n",
    "               vmin=0,\n",
    "               origin='lower',\n",
    "               interpolation='nearest',\n",
    "               aspect=\"auto\")\n",
    "ax.set_yticks(y_all)\n",
    "ax.set_ylabel('bin number (y)')\n",
    "ax.set_xlabel('latent phenotype ($\\phi$)')\n",
    "ax.set_title('measurement process')\n",
    "cb = plt.colorbar(im)\n",
    "cb.set_label('probability  $p(y|\\phi)$', rotation=-90, va=\"bottom\")\n",
    "\n",
    "# Fix up plot\n",
    "fig.tight_layout(w_pad=3)\n",
    "fig.savefig('thermodynmic_custom_gpmap_mpa_visualization_sort_seq.png',bbox_index='tight',dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
